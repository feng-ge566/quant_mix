import torch as t
import numpy as np
import os
import math
LPR8B = [0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 9, 14, 14, 15, 15, 16, 16, 17, 17, 22, 22, 23, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 40, 41, 41, 46, 46, 47, 47, 48, 48, 49, 49, 54, 54, 55, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 65, 65, 66, 66, 67, 67, 68, 68, 69, 69, 70, 70, 71, 71, 72, 72, 72, 72, 73, 73, 73, 73, 78, 78, 78, 78, 79, 79, 79, 79, 112, 112, 112, 112, 113, 113, 113, 113, 118, 118, 118, 118, 119, 119, 119, 119, 120, 120, 121, 121, 122, 122, 123, 123, 124, 124, 125, 125, 126, 126, 127, 127, -128, -128, -127, -127, -126, -126, -125, -125, -124, -124, -123, -123, -122, -122, -121, -121, -120, -120, -120, -120, -119, -119, -119, -119, -114, -114, -114, -114, -113, -113, -113, -113, -80, -80, -80, -80, -79, -79, -79, -79, -74, -74, -74, -74, -73, -73, -73, -73, -72, -72, -71, -71, -70, -70, -69, -69, -68, -68, -67, -67, -66, -66, -65, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -56, -55, -55, -50, -50, -49, -49, -48, -48, -47, -47, -42, -42, -41, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -24, -23, -23, -18, -18, -17, -17, -16, -16, -15, -15, -10, -10, -9, -9, -8, -7, -6, -5, -4, -3, -2, -1]
#LPR8B_fast = [0, 0, 0, 0, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 16, 16, 16, 16, 16, 16, 16, 16, 24, 24, 24, 24, 28, 28, 28, 28, 32, 32, 32, 32, 36, 36, 36, 36, 40, 40, 40, 40, 40, 40, 40, 40, 48, 48, 48, 48, 48, 48, 48, 48, 56, 56, 56, 56, 60, 60, 60, 60, 64, 64, 64, 64, 64, 64, 64, 64, 68, 68, 68, 68, 68, 68, 68, 68, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 120, 120, 120, 120, 120, 120, 120, 120, 124, 124, 124, 124, 124, 124, 124, 124, -128, -128, -128, -128, -128, -128, -128, -128, -124, -124, -124, -124, -124, -124, -124, -124, -120, -120, -120, -120, -120, -120, -120, -120, -120, -120, -120, -120, -120, -120, -120, -120, -80, -80, -80, -80, -80, -80, -80, -80, -80, -80, -80, -80, -80, -80, -80, -80, -72, -72, -72, -72, -72, -72, -72, -72, -68, -68, -68, -68, -68, -68, -68, -68, -64, -64, -64, -64, -60, -60, -60, -60, -56, -56, -56, -56, -56, -56, -56, -56, -48, -48, -48, -48, -48, -48, -48, -48, -40, -40, -40, -40, -36, -36, -36, -36, -32, -32, -32, -32, -28, -28, -28, -28, -24, -24, -24, -24, -24, -24, -24, -24, -16, -16, -16, -16, -16, -16, -16, -16, -8, -8, -8, -8, -4, -4, -4, -4]
#LPR8B_4bit = [0, 1, 1, 2, 2, 4, 4, 4, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,  -4, -4, -4, -2, -2, -2, -1, -1]
LPR8B_4bit = [0, 1, 1, 2, 2, 4, 4, 4, -4, -4, -4, -2, -2, -2, -1, -1]
true_4bit =  [0, 1, 2, 3, 4, 5, 6, 7, -8, -7, -6, -5, -4, -3, -2, -1]
CE_6bit = [0, 1, 2, 2, 4, 4, 6, 7, 8, 9, 10, 10, 12, 12, 14, 15, 16, 16, 17, 17, 18, 18, 18, 18, 28, 28, 28, 28, 30, 30, 31, 31, -32, -32, -31, -31, -30, -30, -30, -30, -20, -20, -20, -20, -18, -18, -17, -17, -16, -15, -14, -14, -12, -12, -10, -9, -8, -7, -6, -6, -4, -4, -2, -1]
# os.environ["CUDA_VISIBLE_DEVICES"] = '1'

pthfile = "/home/liqiufeng/mix_lcd/quant_mix_lr/out/vgg863_70/VGG16_ImageNet_bmix_s0_1bit_best.pth.tar"
net = t.load(pthfile, map_location='cpu')['state_dict']
# net = t.load(pthfile)
# net = t.load(pthfile, map_location={'cuda:0': 'cuda:1'})['state_dict']

fp = open('weight.txt', 'w')
weight_cnt = 0
num = 1
bit = 8


for k, v in net.items():
    if '.weight' in k: 
        # print(k)
        # print(v.size())
        # print(v.shape[1])
        # print(v.shape[2])
        # print(v.shape[3])
        ss = math.log2(0.0033)
        ss = math.floor(ss)
        weight = v / (2**ss)

        weight = weight.round().int().numpy()
        weight = weight.clip(- 2 ** (bit - 1),  2 ** (bit - 1) - 1)
        # zero = np.zeros_like(weight, dtype=None, order='K', subok=True, shape=None)
        dim_data = weight.flatten() 
        #label_data = t.tensor(LPR8B, dtype=t.long).cuda() 
        label_data = np.array(LPR8B)
        data_x = np.take(label_data,  dim_data)           
        # x_cp = data_x.reshape(quan_shape).cuda()   

        weight = data_x
        for d in weight:
            n = str(d)
            fp.write(n + '\n')

# # for conv and fc layer 
# for k, v in net.items():
#     if 'fn.s' in k: 
#         fp.write(k + '\n')
#         ss = math.log2(v)
#         ss = math.floor(ss)

#         # weight = weight.round().int().numpy()
#         # weight = weight.clip(- 2 ** (bit - 1),  2 ** (bit - 1) - 1)
#         # # zero = np.zeros_like(weight, dtype=None, order='K', subok=True, shape=None)
#         # dim_data = weight.flatten() 
#         # #label_data = t.tensor(LPR8B, dtype=t.long).cuda() 
#         # label_data = np.array(LPR8B)
#         # data_x = np.take(label_data,  dim_data)           
#         # # x_cp = data_x.reshape(quan_shape).cuda()   

#         # weight = data_x

#         n = str(ss)
#         fp.write(n + '\n')



        # print(k)
        # print(v)
    # with open('scale.txt', 'a') as file1:
    #     print(net, file=file1)

# with open('test.txt', 'a') as file0:
#     print(net, file=file0)


